<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Oira.AI</title>
    <link rel="icon" href="media/logo.webp">
    <meta name="description" content="Oira, AI, Oira.AI">
    <style>
      @font-face {
        font-family: "Fragmentcore";
        src: url("fonts/Fragmentcore.otf");
      }

      @font-face {
        font-family: "Roboto";
        src: url("fonts/Roboto-Regular.ttf");
      }

      ::-webkit-scrollbar {
        width: 11px;
      }

      ::-webkit-scrollbar-track {
        border: solid 3px transparent;
      }

      ::-webkit-scrollbar-thumb {
        box-shadow: inset 0 0 10px 10px white;
        border: solid 3px transparent;
        border-radius: 25px;
      }

      body {
        margin: 0;
        background-color: #000;
      }

      h1 {
        color: #fff;
      }

      main {
        width: 100%;
      }

      .tech {
        display: flex;
        flex-direction: column;
        justify-content: flex-start;
        font-family: "Roboto", arial, sans-serif;
        color: #fff;
        padding: 20px;
      }

      .MNB, .VITS {
        width: 50dvw;
        margin: auto;
        height: auto;
        margin-bottom: 50px;
      }

      img {
        width: 100%;
      }
    </style>
  </head>
  <body>
    <main>
        <div class="tech">
		<div class="MNB">
        <h1 style="text-align: center;">Multinomial Naive Bayes</h1>
        <h3>What is the Multinomial Naive Bayes algorithm?</h3>
        <p>The Multinomial Naive Bayes algorithm is a probabilistic learning method widely applied in Natural Language Processing (NLP). It leverages the principles of the Bayes theorem to predict the class (e.g., language) of a text document, email, or other textual data. It calculates the probability distribution of each class for a given document and assigns the class with the highest probability as the prediction.</p>
        <h3>How does it work?</h3>
        <p>The Multinomial Naive Bayes algorithm is specifically designed for text classification tasks. Formulated based on the Bayes theorem, originally proposed by Thomas Bayes, it computes the probability of an event occurring given the prior knowledge of conditions associated with the event. The algorithm is governed by the following formula:</p>
        <p><b><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>A</mi><mi mathvariant="normal">∣</mi><mi>B</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>B</mi><mi mathvariant="normal">∣</mi><mi>A</mi><mo stretchy="false">)</mo><mo>⋅</mo><mi>P</mi><mo stretchy="false">(</mo><mi>A</mi><mo stretchy="false">)</mo></mrow><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>B</mi><mo stretchy="false">)</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)} 
        </annotation></semantics></math></b></p>
        <p>Here, we evaluate the probability of class A given the presence of predictor B.</p>
        <ul>
        <li><b>P(B)</b>: Prior probability of predictor B</li>
        <li><b>P(A)</b>: Prior probability of class A</li>
        <li><b>P(B|A)</b>: Conditional probability of predictor B given class A</li>
        </ul>
        <img loading="lazy" draggable="false" src="media/Multinomial Naive Bayes - Visual.webp">
        <p>The image likely includes a visual representation of the Naive Bayes Classifier at work. It might show data points, represented as shapes or colors, being classified into different categories.</p>
        <p>The “naive” aspect comes from the assumption that all features are independent of each other within each class. Despite this simplification, Naive Bayes Classifiers work well in many real-world situations, especially for text classification and spam filtering.</p>
        <p>
          All in all, the Multinomial Naive Bayes algorithm is a powerful tool for text classification in Natural Language Processing. Leveraging the Bayes theorem, it calculates the probability distribution of each class for a given text document and assigns the class with the highest probability as the prediction. Despite its "naive" assumption of feature independence within each class, it proves effective in real-world scenarios, particularly in tasks like text classification and spam filtering. Its simplicity and efficiency make it a popular choice for various NLP applications.</p>
		  </div>
      <div class="VITS">
        <h1 style="text-align: center;">VITS</h1>
        <h3>What is VITS?</h3>
        <p>Vits, also known as "Variational Inference with adversarial
          learning for end-to-end Text-to-Speech", is a</p>
      </div>
        </div>
    </main>
    </body>
</html>